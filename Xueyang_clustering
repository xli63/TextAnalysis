import pandas as pd
import numpy as np

from nrclex import NRCLex
import spacy
import nltk
import re
import string
import contractions
import gensim
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans

nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
import nltk.data

#read in data from noah's github
url = 'https://raw.githubusercontent.com/nrsundberg/TextAnalysis/main/Script.csv'
script = pd.read_csv(url, index_col=0)
(script.head(5))

script['character'].unique()

#rename characters that were renamed from data scrape
#name_mapping = {'Joyce': "JOY", 'Sandra': "SADNESS", 'Felipe':"FEAR", 'Diane':"DISGUST"
                                              #, 'Angelo': "ANGER"}
#script.replace(name_mapping, inplace=True)


#see how many lines are spoken by each character
#script['character'].value_counts()

# clustering

script.head()

#get into a string for the key characters
script2 = script

script2.loc[script2['character'].isin(['RILEY', 'YOUNG RILEY', 'RILEY VOICE']), 'character'] = 'RILEY'

#concats all characters quotes in its dataframe row 
script2['text'] = script[['character','quote']].groupby(['character'])['quote'].transform(lambda x: ' '.join(x))
script2= script2[['character', 'text']].drop_duplicates()    

#create lists for each main character
Joyce = ""
Sandra = ""
Felipe = ""
Diane = ""
Angelo = ""
Riley = ""

for character in script2['character']: 
    if character== 'Joyce': 
        Joyce+=(script2[script2['character']== character]['text'].item())
    elif character== 'Sandra': 
        Sandra+=(script2[script2['character']== character]['text'].item())
    elif character== 'Felipe': 
        Felipe+=(script2[script2['character']== character]['text'].item())
    elif character== 'Diane': 
        Diane+=(script2[script2['character']== character]['text'].item())
    elif character== 'Angelo': 
        Angelo+=(script2[script2['character']== character]['text'].item())
    elif character== 'RILEY': 
        Riley+=(script2[script2['character']== character]['text'].item())

#some basic cleaning: 

char_strings= [Joyce, Sandra, Felipe, Diane, Angelo, Riley]

for i in range(0,len(char_strings)): 
    #fix contractions 
    char_str = char_strings[i]
    char_str= contractions.fix(char_str)
    
    #remove punctuation- for term vector
    punc = re.compile( '[%s]' % re.escape( string.punctuation ) ) 
    
    #lowercase
    low = char_str.lower()
    
    #remove text in parentheses
    low = re.sub("[\(\[].*?[\)\]]", "", low)
    
    char_strings[i] = low
    
    #break back out into separate
    Joyce, Sandra, Felipe, Diane, Angelo, Riley = char_strings

#create term vectors (each position in)
term_vec = [ ]

for d in char_strings:
    d = d.lower()
    d = punc.sub( '', d )
    term_vec.append( nltk.word_tokenize( d ) )

# Print resulting term vectors

for vec in term_vec:
    print(vec)

joyce_tv, sandra_tv, felipe_tv, diane_tv, angelo_tv, riley_tv = term_vec

# Remove stop words from term vectors

stop_words = nltk.corpus.stopwords.words( 'english' )

for i in range( 0, len( term_vec ) ):
    term_list = [ ]

    for term in term_vec[ i ]:
        if term not in stop_words:
            term_list.append( term )

    term_vec[ i ] = term_list

# Print term vectors with stop words removed

for vec in term_vec:
    print(vec)
    
#term vectors with no stop words
joyce_tv1, sandra_tv1, felipe_tv1, diane_tv1, angelo_tv1, riley_tv1 = term_vec

#porter stemming
def porter_stem( txt ):
    """Porter stem terms in text block

    ... Args:
    ...   txt (list of string): Text block as list of individual terms
    ...
    ... Returns:
    ...   (list of string): Text block with terms Porter stemmed
    ... """
    porter = nltk.stem.porter.PorterStemmer()

    for i in range( 0, len( txt ) ):
        txt[ i ] = porter.stem( txt[ i ][j] )
        return txt

#test porter stemming
term_vec_stem = []
for i in range( 0, len( term_vec ) ):
    for j in range(0, len(term_vec[i])): 
        porter = nltk.stem.porter.PorterStemmer()
        stemmed = porter.stem(term_vec[i][j])
        print(stemmed)
        
#i'm not loving the porter stemming 

dict = gensim.corpora.Dictionary( term_vec )

corp = [ ]
for i in range( 0, len( term_vec ) ):
    corp.append( dict.doc2bow( term_vec[ i ] ) )

#  Create TFIDF vectors based on term vectors bag-of-word corpora

tfidf_model = gensim.models.TfidfModel( corp )

tfidf = [ ]
for i in range( 0, len( corp ) ):
    tfidf.append( tfidf_model[ corp[ i ] ] )

#  Create pairwise document similarity index

n = len( dict )
index = gensim.similarities.SparseMatrixSimilarity( tfidf_model[ corp ], num_features = n )

#  Print TFIDF vectors and pairwise similarity per document

for i in range( 0, len( tfidf ) ):
    s = 'Doc ' + str( i + 1 ) + ' TFIDF:'

    for j in range( 0, len( tfidf[ i ] ) ):
        s = s + ' (' + dict.get( tfidf[ i ][ j ][ 0 ] ) + ','
        s = s + ( '%.3f' % tfidf[ i ][ j ][ 1 ] ) + ')'

    print(s)

for i in range( 0, len( corp ) ):
    print('Doc', ( i + 1 ), 'sim: [ '),

    sim = index[ tfidf_model[ corp[ i ] ] ]
    for j in range( 0, len( sim ) ):
        print('%.3f ' % sim[ j ]),

    print(']')

#attempt healeys clustering - kmeans with k=4 from elbow function
txt = char_strings

full_sent = [ ]
for i in range( 0, len( txt ) ):
    sent = re.sub( r'[\.!\?]"', '"', txt[ i ] )
    full_sent += re.split( '[\.!\?]', sent )
    full_sent = [sent.strip() for sent in full_sent]


# Remove empty sentences
i = 0
while i < len( full_sent ):
    if len( full_sent[ i ] ) == 0:
        del full_sent[ i ]
    else:
        i += 1

# Remove punctuation
sent = [ ]
punc = string.punctuation.replace( '-', '' )
for i in range( 0, len( full_sent ) ):
    sent.append( re.sub( '[' + punc + ']+', '', full_sent[ i ] ) )
    
# Remove empty sentences after stop word removal
i = 0
while i < len( sent ):
    if len( sent[ i ] ) == 0:
        del sent[ i ]
    else:
        i += 1
        

tfidf = TfidfVectorizer( stop_words='english', max_df=0.8, max_features=1000 )
term_vec = tfidf.fit_transform( sent )
X = cosine_similarity( term_vec )

# Fit vectors to clusters (4 clusters)
clust = KMeans( n_clusters=4, random_state=1 ).fit( X )
print( clust.labels_ )

for i in range( 0, len( set( clust.labels_ ) ) ):
    print( f'Cluster {i}:' )

    for j in range( 0, len( clust.labels_ ) ):
        if clust.labels_[ j ] == i:
            print( full_sent[ j ].replace( '"', '' ).strip() )
    print()
    print()
    print()

#determine no. clusters (k) 

import numpy as np
from scipy.spatial.distance import cdist

def elbow( X, max_clust=25 ):
    distort = [ ]
    inertia = [ ]

    map_distort = { }
    map_inertia = { }

    elbow_distort = 1
    elbow_inertia = 1

    K = range( 1, max_clust )
    for k in K:
        kmean_model = KMeans( n_clusters=k )
        kmean_model.fit( X )

        distort.append( sum( np.min( cdist( X, kmean_model.cluster_centers_, 'euclidean' ), axis=1 ) ) / X.shape[ 0 ] )
        inertia.append( kmean_model.inertia_ )

        map_distort[ k ] = sum( np.min( cdist( X, kmean_model.cluster_centers_, 'euclidean' ), axis=1 ) ) / X.shape[ 0 ]
        map_inertia[ k ] = kmean_model.inertia_

    prev_k = ''
    prev_v = 0
    prev_pct = 0
    for i,(k,v) in enumerate( map_distort.items() ):
        if prev_k == '':
            print( f'{k}: {v:.4f}' )
            prev_k = str( k )
            prev_v = v
            continue

        print( f'{k}: {v:.4f} ', end='' )

        diff_v = prev_v - v
        diff_v_pct = diff_v / prev_v * 100.0
        print( f'{diff_v:.4f}, {diff_v_pct:.2f}%' )

        if i > 2 and prev_pct - diff_v_pct < 0.5:
            elbow_distort = i + 1
            break

        prev_k = str( k )
        prev_v = v
        prev_pct = diff_v_pct

    print()

    prev_k = ''
    prev_v = 0
    prev_pct = 0
    for i,(k,v) in enumerate( map_inertia.items() ):
        if prev_k == '':
            print( f'{k}: {v:.4f}' )
            prev_k = str( k )
            prev_v = v
            continue

        print( f'{k}: {v:.4f} ', end='' )

        diff_v = prev_v - v
        diff_v_pct = diff_v / prev_v * 100.0
        print( f'{diff_v:.4f}, {diff_v_pct:.2f}%' )

        if i > 2 and prev_pct - diff_v_pct < 0.5:
            elbow_inertia = i + 1
            break

        prev_k = str( k )
        prev_v = v
        prev_pct = diff_v_pct

    return max( elbow_distort, elbow_inertia )


#test- seems like 3 or 4 is best (4) 
elbow(X)

import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.feature_extraction.text import CountVectorizer

# Count raw term frequencies

count = CountVectorizer( stop_words='english' )
term_vec = count.fit_transform( sent )

n_topic = 5

# Build a string list of [ 'Topic 1', 'Topic 2', ..., 'Topic n' ]

col_nm = [ ]
for i in range( 1, n_topic + 1 ):
    col_nm += [ f'Topic {i}' ]

# Fit an LDA model to the term vectors, get cosine similarities

lda_model = LDA( n_components=n_topic )
concept = lda_model.fit_transform( term_vec )
X = cosine_similarity( concept )

# Print top 10 terms for each topic

feat = count.get_feature_names()
topic_list = [ ]
for i,topic in enumerate( lda_model.components_ ):
    top_n = [ feat[ i ] for i in topic.argsort()[ -10: ] ]
    top_feat = ' '.join( top_n )
    topic_list.append( f"topic_{'_'.join(top_n[ :3 ] ) }" )

    print( f'Topic {i}: {top_feat}' )
print()

# Cluster sentences and print clusters (k=4)

clust = KMeans( n_clusters=4 ).fit( concept )

for i in range( 0, len( set( clust.labels_ ) ) ):
    print( f'Cluster {i}:' )
    for j in range( 0, len( clust.labels_ ) ):
        if clust.labels_[ j ] != i:
            continue
        print( full_sent[ j ] )

    print()


##IGNORE- I WAS TESTING STUFF 

#split terms

##Creates separate dataframes for each character's dialogue
joyce = script[script['character'] == 'Joyce']
sandra = script[script['character'] == 'Sandra']
felipe = script[script['character'] == 'Felipe']
diane = script[script['character'] == 'Diane']
angelo = script[script['character'] == 'Angelo']
riley = script[script['character'].isin(['YOUNG RILEY', 'RILEY VOICE', 'RILEY'])]   #Could be interesting to see the difference between young Riley and Riley.
riley['character']= "RILEY"

##Function used to convert the dataframe name to a string
def get_df_name(df):
    name =[x for x in globals() if globals()[x] is df][0]
    return name

##For each character, pass all of their dialogue through NRCLex and get back an 'affect frequency' for each word.  Then sums up the weighted frequencies of all words.
characters = ['Joyce', 'Sandra', 'Felipe', 'Diane', 'Angelo', 'Riley']


text_frame = {}
up_char= [x.upper() for x in characters]
for character in up_char: 
    text_frame[character]= []
    
for character in (joyce, sandra, felipe, diane, angelo, riley ):

    char_name = (character['character']).tolist()[0]

    for line in character['quote']:

        #handle contractions
        line= contractions.fix(line)

        #remove punctuation- for term vector
        punc = re.compile( '[%s]' % re.escape( string.punctuation ) ) 
        
        low = line.lower()
        punc = punc.sub( ' ', line)
                
        #create dic
        key = char_name.upper()
        value = (punc)
        text_frame[key].append(value)        


### run derek's code

from nrclex import NRCLex
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
import pandas as pd

#print(script['character'].unique())
#print(script['character'].value_counts())

##Creates separate dataframes for each character's dialogue
joyce = script[script['character'] == 'Joyce']
sandra = script[script['character'] == 'Sandra']
felipe = script[script['character'] == 'Felipe']
diane = script[script['character'] == 'Diane']
angelo = script[script['character'] == 'Angelo']
riley = script[script['character'].isin(['YOUNG RILEY', 'RILEY VOICE', 'RILEY'])]   #Could be interesting to see the difference between young Riley and Riley.

##Function used to convert the dataframe name to a string
def get_df_name(df):
    name =[x for x in globals() if globals()[x] is df][0]
    return name

##For each character, pass all of their dialogue through NRCLex and get back an 'affect frequency' for each word.  Then sums up the weighted frequencies of all words.
characters = ['Joyce', 'Sandra', 'Felipe', 'Diane', 'Angelo', 'Riley']
overall = []
for character in (joyce, sandra, felipe, diane, angelo, riley):
    data = []
    for line in character['quote']:
        words = word_tokenize(line)     #Breaks the string into a list of its words
        for term in words:
            emotion = NRCLex(term)
            if len(emotion.affect_list) > 0:    #If the selected word has no emotion association, then the word will be ignored
                temp = emotion.affect_frequencies
                temp['term'] = term     #Adds the selected word with its affect frequency
                data.append(temp)
    df_nrc = pd.DataFrame.from_dict(data)
    df_nrc = df_nrc.fillna(0)  
    temp2 = {}  #Temporary dictionary to add to the final dataframe.  Definitely could be a more efficient way to do this.
    temp2['character'] = get_df_name(character)     #Character's name
    temp2['Anger'] = sum(df_nrc['anger']) / len(df_nrc)     #The sum of all anger-weighted words for a character and divide by the number of quotes since characters have different numbers of lines.  Another possibility could be to divide by total words spoken for each character.
    temp2['Disgust'] = sum(df_nrc['disgust']) / len(df_nrc)
    temp2['Fear'] = sum(df_nrc['fear']) / len(df_nrc)
    temp2['Joy'] = sum(df_nrc['joy']) / len(df_nrc)
    temp2['Sadness'] = sum(df_nrc['sadness']) / len(df_nrc)
    temp2['Positive'] = sum(df_nrc['positive']) / len(df_nrc)
    temp2['Negative'] = sum(df_nrc['negative']) / len(df_nrc)
    temp2['Trust'] = sum(df_nrc['trust']) / len(df_nrc)
    temp2['Surprise'] = sum(df_nrc['surprise']) / len(df_nrc)
    temp2['Anticipation'] = (sum(df_nrc['anticipation']) + sum(df_nrc['anticip'])) / len(df_nrc)    #For some reason NRCLex created an 'anticip' value for some words
    overall.append(temp2)
df = pd.DataFrame.from_dict(overall)
print(df.head(6))   #Final dataframe has the emotional breakdown for each character based on the 10 possible values.  All rows should add to 1.



